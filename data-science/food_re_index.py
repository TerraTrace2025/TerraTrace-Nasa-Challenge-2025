# -*- coding: utf-8 -*-
"""food_re_index

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVtYxmKUGSKIzjV9mnjoi6WByGPW4uCH

!pip install geocoder
!pip install meteostat
!pip install geopandas
!pip install osmnx
"""

import os
import requests
from datetime import datetime, timedelta, date
import pandas as pd
from google.colab import files
import geocoder
from geopy.geocoders import Nominatim
from tqdm import tqdm
import geopandas as gpd
import osmnx as ox
from shapely.geometry import Point
import tkinter as tk
from tkinter import messagebox
import ipywidgets as widgets
from IPython.display import display
from dateutil import parser
import ipywidgets as widgets
from IPython.display import display
from meteostat import Stations, Daily
import ee
import geemap

N = 14 # we want to see the risk index in the future 7 days
 year = 2022
 loc_num= 10

"""***Input*** ***1: Crop Information: Names & Locations***"""

df = pd.read_excel("harvest_locations.xlsx", skiprows=1)

# state_location = df["Kt."]
# origin_location = df["Standort"]
crop_names = df.columns[7:].tolist()

mapping = {
    'Brotweizen': 'Wheat',
    'Umstellmahlweizen': 'Wheat',
    'Futterweizen': 'Wheat',
    'Futtergerste': 'Barley',
    'K√∂rnermais': 'Corn',
    'Sojabohnen Futter': 'Soybean',
    'Sojabohnen "Tofu"': 'Soybean',
    'Raps Kl.': 'Rapeseed',
    'Raps HOLL': 'Rapeseed',
    'Sonnenblumen lino': 'Sunflowerseed',
    'Sonnenblumen HO': 'Sunflowerseed'
}

swiss_crop = [crop for crop in crop_names if crop in mapping]
locations_dict = {}

for crop in swiss_crop:
    category = mapping[crop]
    crop_locations = df.loc[df[crop] == "x", "Standort"].tolist()
    crop_locations = crop_locations[:loc_num]
    if category not in locations_dict:
        locations_dict[category] = crop_locations
    else:
        combined = list(dict.fromkeys(locations_dict[category] + crop_locations))
        locations_dict[category] = combined[:loc_num]
for k, v in locations_dict.items():
    print(f"{k}: {v}")
    print(f"{k}: {len(v)} locations")

selected_crop = ""

def confirm_selection(b):
    global selected_crop
    selected_crop = dropdown.value

dropdown = widgets.Dropdown(
    options=['Wheat', 'Barley', 'Corn', 'Soybean', 'Rapeseed', 'Sunflowerseed'],
    description='Select the crop and check its risk index in {} days:'.format(N)
)

button = widgets.Button(description="Confirm Selection")
button.on_click(confirm_selection)

display(dropdown, button)

print("Risk in index for", selected_crop, "in {} days".format(N))

geolocator = Nominatim(user_agent="geoapi", timeout=10)

cache = {}

def clean_location(address):
    address = address.replace("b.", "bei")
    if "(FL)" in address:
        address = address.replace("(FL)", "").strip()
        country = "Liechtenstein"
    else:
        country = "Switzerland"
    return address, country

def get_lat_lon_cached(address):
    if address in cache:
        return cache[address]

    try:
        clean_addr, country = clean_location(address)
        location = geolocator.geocode(f"{clean_addr}, {country}")
        if location:
            result = (location.latitude, location.longitude)
        else:
            result = (None, None)
    except:
        result = (None, None)

    manual_fix = {
        "3292 Busswil bei B√ºren": (47.133, 7.246),
        "9494 Schaan": (47.166, 9.516)
    }
    if result == (None, None) and clean_addr in manual_fix:
        result = manual_fix[clean_addr]

    cache[address] = result
    return result

print(f"Processing {selected_crop}...")

coords_list = []
for loc in tqdm(locations_dict[selected_crop]):
    lat, lon = get_lat_lon_cached(loc)
    coords_list.append([loc, lat, lon])

selected_crop_loc = pd.DataFrame(coords_list, columns=["Standort", "Latitude", "Longitude"])

print(" ")
print("The locations of farms in Switzerland for {}".format(selected_crop))
print(" ")
print(selected_crop_loc)

"""***Input 2: Climate Data***

* Historical climate data
"""

weather_rows = []
columns_keep = ["tavg", "tmin", "tmax", "prcp", "snow", "wspd", "wpgt"]

for idx, row in selected_crop_loc.iterrows():
    lat, lon = row["Latitude"], row["Longitude"]
    start = datetime(year, 1, 1)
    end = datetime(year, 12, 31)
    stations = Stations().nearby(lat, lon).fetch(10)
    if stations.empty:
        print(f"No stations found for {lat},{lon}")
        continue
    station_means = []
    for sid in stations.index:
        try:
            data = Daily(sid, start, end).fetch()
            if data.empty:
                continue
            yearly_mean = data[columns_keep].mean()
            station_means.append(yearly_mean)
        except Exception as e:
            print(f"Failed for station {sid}: {e}")
    if not station_means:
        continue
    combined_mean = pd.concat(station_means, axis=1).mean(axis=1, skipna=True)
    weather_data = {
        "Standort": row["Standort"],
        "Latitude": lat,
        "Longitude": lon
    }
    weather_data.update(combined_mean.to_dict())
    weather_rows.append(weather_data)
    if len(weather_rows) >= loc_num:
        break

weather_df = pd.DataFrame(weather_rows)
if "snow" in weather_df.columns:
    weather_df["snow"].fillna(0.025, inplace=True)

print(weather_df)

"""***Collection of extrem weather days***

OPEN_METEO_ARCHIVE = "https://archive-api.open-meteo.com/v1/archive"

THRESH = {
    "heavy_rain_mm": 50.0,
    "heavy_snow_cm": 20.0,
    "heatwave_temp_c": 35.0,
    "strong_wind_ms": 15.0,
    "extreme_uv_index": 8.0,
    "dry_day_precip_mm": 1.0,
    "dry_fraction_for_drought": 0.8,
}

def get_yearly_weather_and_extremes(lat, lon, year, thresholds=None, timezone="Europe/Zurich"):
    if thresholds is None:
        thresholds = THRESH
    else:
        tmp = THRESH.copy()
        tmp.update(thresholds)
        thresholds = tmp

    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_date,
        "end_date": end_date,
        "daily": ",".join([
            "temperature_2m_max","temperature_2m_min",
            "precipitation_sum","snowfall_sum",
            "wind_speed_10m_max","uv_index_max"
        ]),
        "timezone": timezone
    }

    r = requests.get(OPEN_METEO_ARCHIVE, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    daily = data.get("daily", {})
    if not daily:
        raise ValueError("Open-Meteo returned no daily data for the given coordinates/year.")

    df = pd.DataFrame(daily)

    mean_data = {
        "mean_temperature_2m_max": df["temperature_2m_max"].mean(),
        "mean_temperature_2m_min": df["temperature_2m_min"].mean(),
        "mean_precipitation_sum": df["precipitation_sum"].mean(),
        "mean_snowfall_sum": df["snowfall_sum"].mean(),
        "mean_wind_speed_10m_max": df["wind_speed_10m_max"].mean(),
        "mean_uv_index_max": df["uv_index_max"].mean()
    }

    df["extreme_uv"] = df["uv_index_max"] >= thresholds["extreme_uv_index"]
    df["heavy_rain"] = df["precipitation_sum"] >= thresholds["heavy_rain_mm"]
    df["heavy_snow"] = df["snowfall_sum"]/10 >= thresholds["heavy_snow_cm"]  # ËΩ¨‰∏∫cm
    df["extreme_heat"] = df["temperature_2m_max"] >= thresholds["heatwave_temp_c"]
    df["strong_wind"] = df["wind_speed_10m_max"] >= thresholds["strong_wind_ms"]
    df["dry_day"] = df["precipitation_sum"] < thresholds["dry_day_precip_mm"]

    total_days = len(df)
    dry_days = int(df["dry_day"].sum())
    dry_fraction = dry_days / total_days if total_days > 0 else None
    simple_drought_flag = dry_fraction is not None and dry_fraction >= thresholds["dry_fraction_for_drought"]

    extreme_summary = {
        "period_days": total_days,
        "heavy_rain_days": int(df["heavy_rain"].sum()),
        "heavy_snow_days": int(df["heavy_snow"].sum()),
        "extreme_uv_days": int(df["extreme_uv"].sum()),
        "extreme_heat_days": int(df["extreme_heat"].sum()),
        "strong_wind_days": int(df["strong_wind"].sum()),
        "dry_days": dry_days,
        "dry_fraction": dry_fraction,
        "simple_drought_detected": bool(simple_drought_flag)
    }

    result = {**mean_data, **extreme_summary}
    return result

summary_rows = []
for idx, row in selected_crop_loc.iterrows():
    try:
        weather_summary = get_yearly_weather_and_extremes(
            lat=row["Latitude"], lon=row["Longitude"], year=year
        )
        row_data = {
            "Standort": row["Standort"],
            "Latitude": row["Latitude"],
            "Longitude": row["Longitude"]
        }
        row_data.update(weather_summary)
        summary_rows.append(row_data)
        time.sleep(1)
    except requests.exceptions.HTTPError as e:
        print(f"Request failed for {row['Standort']}: {e}")
        time.sleep(1)

extreme_weather_df = pd.DataFrame(summary_rows)
print(extreme_weather_df)

***Weather forecast in the future N days***
"""

FORECAST_URL = "https://api.open-meteo.com/v1/forecast"
OPEN_METEO_FORECAST = "https://api.open-meteo.com/v1/forecast"

def get_forecast_weather(latitude, longitude, n_days):
    start_date = date.today()
    end_date = start_date + timedelta(days=n_days - 1)

    daily_vars = [
        "temperature_2m_max","temperature_2m_min",
        "precipitation_sum","windspeed_10m_max",
        "weathercode","snowfall_sum"
    ]

    params = {
        "latitude": latitude,
        "longitude": longitude,
        "daily": ",".join(daily_vars),
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),
        "timezone": "Europe/Zurich"
    }

    resp = requests.get(FORECAST_URL, params=params, timeout=30)
    resp.raise_for_status()

    df = pd.DataFrame(resp.json()["daily"])
    df["time"] = pd.to_datetime(df["time"]).dt.date
    def code_to_simple(wc, precip, wind, snow):
        if snow > 0:
            return "snow"
        elif precip > 5:
            return "rain"
        elif precip > 0 and wind > 10:
            return "showers/windy"
        elif wc in [0,1,2]:  # clear/sunny
            return "sunny"
        else:
            return "cloudy"

    df["weather_simple"] = df.apply(
        lambda row: code_to_simple(
            row["weathercode"],
            row["precipitation_sum"],
            row["windspeed_10m_max"],
            row.get("snowfall_sum", 0)
        ),
        axis=1
    )

    return df

THRESHOLDS = {
    "heatwave_temp_c": 35.0,
    "strong_wind_ms": 15.0,
    "heavy_rain_mm": 50.0,
    "heavy_snow_cm": 20.0
}

def extreme_weather_alert(lat, lon, n_days=N, timezone="Europe/Zurich"):
    start_date = date.today()
    end_date = start_date + timedelta(days=n_days-1)

    params = {
        "latitude": lat,
        "longitude": lon,
        "daily": "temperature_2m_max,precipitation_sum,snowfall_sum,windspeed_10m_max",
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),
        "timezone": timezone
    }

    resp = requests.get(OPEN_METEO_FORECAST, params=params)
    resp.raise_for_status()
    data = resp.json()

    df = pd.DataFrame(data["daily"])

    alerts = []
    for i, row in df.iterrows():
        msg = []
        if row["temperature_2m_max"] >= THRESHOLDS["heatwave_temp_c"]:
            msg.append("üî• Heatwave")
        if row["windspeed_10m_max"] >= THRESHOLDS["strong_wind_ms"]:
            msg.append("üí® Strong wind")
        if row["precipitation_sum"] >= THRESHOLDS["heavy_rain_mm"]:
            msg.append("üåßÔ∏è Heavy rain")
        if (row["snowfall_sum"] / 10.0) >= THRESHOLDS["heavy_snow_cm"]:  # mm‚Üícm
            msg.append("‚ùÑÔ∏è Heavy snow")

        alerts.append(", ".join(msg) if msg else "üòÉ NA")

    df["extreme_alert"] = alerts
    return df

forecast_rows = []
for idx, row in selected_crop_loc.iterrows():
    lat, lon = row["Latitude"], row["Longitude"]
    standort = row["Standort"]
    try:
        df_forecast = get_forecast_weather(lat, lon, N)
        df_alert = extreme_weather_alert(lat, lon, N)
        df_merged = pd.merge(df_forecast, df_alert[["time", "extreme_alert"]], on="time", how="left")
        df_merged["Standort"] = standort
        df_merged["Latitude"] = lat
        df_merged["Longitude"] = lon
        forecast_rows.append(df_merged)
        time.sleep(1)
    except Exception as e:
        print(f"Forecast failed for {standort}: {e}")
        time.sleep(1)

forecast_all = pd.concat(forecast_rows, ignore_index=True)
forecast_all = forecast_all.drop(columns=["Latitude", "Longitude", "weathercode"])
cols = ["Standort"] + [c for c in forecast_all.columns if c != "Standort"]
forecast_all = forecast_all[cols]

"""***Input 3: Yield Data (Optional but Recommended)***"""

df = pd.read_csv("FAOSTAT_data.csv")

mapping = {
    "Wheat": "Wheat",
    "Barley": "Barley",
    "Maize (corn)": "Corn",
    "Green corn (maize)": "Corn",
    "Corn": "Corn",
    "Soya beans": "Soya",
    "Soya bean oil": "Soya",
    "Rapeseed or canola oil, crude": "Rapeseed",
    "Sunflower seed": "Sunflower",
    "Sunflower-seed oil, crude": "Sunflower",
    "Beer of barley, malted": "Barley"
}


keywords = ["Wheat", "Barley", "Corn", "Soya", "Rapeseed", "Sunflower"]
mask = df["Item"].str.contains('|'.join(keywords), case=False, na=False)
df_filtered = df[mask]

columns_to_keep = ["Area", "Item", "Year", "Unit", "Value"]
df_filtered = df_filtered[columns_to_keep]

df_filtered_t = df_filtered[df_filtered["Unit"] == "t"].copy()

df_filtered_t["Item_clean"] = df_filtered_t["Item"].map(mapping)

df_final = df_filtered_t.groupby(["Year", "Item_clean"], as_index=False)["Value"].sum()
value_row = df_final[(df_final["Year"] == year) & (df_final["Item_clean"] == selected_crop)]
value = value_row["Value"].iloc[0]
print(value)

"""***Input 4: Administrative or grid boundaries (crop location)***

"""

ee.Authenticate()
ee.Initialize(project='trusty-sentinel-473919-a6')

def compute_ndvi_df(selected_crop_loc, selected_crop, year, total_yield=value, buffer_m=500):
    """
    selected_crop_loc: pd.DataFrame with columns Standort, Latitude, Longitude
    selected_crop: str, e.g., "Corn"
    year: int
    total_yield: float, total yield for this crop
    buffer_m: int, buffer in meters for NDVI averaging
    Returns: pd.DataFrame with Standort, crop, ndvi, estimated_yield
    """
    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    all_points = ee.Geometry.MultiPoint([[row["Longitude"], row["Latitude"]] for idx, row in selected_crop_loc.iterrows()])

    collection = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
                  .filterDate(start_date, end_date)
                  .filterBounds(all_points)
                  .map(lambda img: img.normalizedDifference(['B8', 'B4']).rename('NDVI')))

    results = []
    for idx, row in selected_crop_loc.iterrows():
        point = ee.Geometry.Point([row["Longitude"], row["Latitude"]])
        ndvi_ts = collection.mean().reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=point.buffer(buffer_m),
            scale=10
        )
        ndvi_value = ndvi_ts.get("NDVI").getInfo()
        results.append({
            "Standort": row["Standort"],
            "crop": selected_crop,
            "ndvi": ndvi_value
        })

    valid_points = [r for r in results if r["ndvi"] is not None]
    if valid_points:
        ndvi_sum = sum(r["ndvi"] for r in valid_points)
        for r in valid_points:
            r["estimated_yield"] = (r["ndvi"] / ndvi_sum) * total_yield
    else:
        print("‚ö†Ô∏è All NDVI values are None!")

    return pd.DataFrame(results)

df_ndvi = compute_ndvi_df(selected_crop_loc, selected_crop, year=year, total_yield=value)
print(df_ndvi)