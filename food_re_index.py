# -*- coding: utf-8 -*-
"""food_re_index

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVtYxmKUGSKIzjV9mnjoi6WByGPW4uCH

!pip install geocoder
!pip install meteostat
!pip install geopandas
!pip install osmnx
"""

import os
import requests
from datetime import datetime, timedelta, date
import pandas as pd
from google.colab import files
import geocoder
from geopy.geocoders import Nominatim
from tqdm import tqdm
import geopandas as gpd
import osmnx as ox
from shapely.geometry import Point
import tkinter as tk
from tkinter import messagebox
import ipywidgets as widgets
from IPython.display import display
from dateutil import parser
import ipywidgets as widgets
from IPython.display import display
from meteostat import Stations, Daily

N = 14 # we want to see the risk index in the future 7 days
 year = 2022
 loc_num= 5

"""***Input*** ***1: Crop Information: Names & Locations***"""

df = pd.read_excel("harvest_locations.xlsx", skiprows=1)

# state_location = df["Kt."]
# origin_location = df["Standort"]
crop_names = df.columns[7:].tolist()

mapping = {
    'Brotweizen': 'Wheat',
    'Umstellmahlweizen': 'Wheat',
    'Futterweizen': 'Wheat',
    'Futtergerste': 'Barley',
    'Körnermais': 'Corn',
    'Sojabohnen Futter': 'Soybean',
    'Sojabohnen "Tofu"': 'Soybean',
    'Raps Kl.': 'Rapeseed',
    'Raps HOLL': 'Rapeseed',
    'Sonnenblumen lino': 'Sunflowerseed',
    'Sonnenblumen HO': 'Sunflowerseed'
}

swiss_crop = [crop for crop in crop_names if crop in mapping]
locations_dict = {}

for crop in swiss_crop:
    category = mapping[crop]
    crop_locations = df.loc[df[crop] == "x", "Standort"].tolist()
    crop_locations = crop_locations[:loc_num]
    if category not in locations_dict:
        locations_dict[category] = crop_locations
    else:
        combined = list(dict.fromkeys(locations_dict[category] + crop_locations))
        locations_dict[category] = combined[:loc_num]
for k, v in locations_dict.items():
    print(f"{k}: {v}")
    print(f"{k}: {len(v)} locations")

selected_crop = ""

def confirm_selection(b):
    global selected_crop
    selected_crop = dropdown.value

dropdown = widgets.Dropdown(
    options=['Wheat', 'Barley', 'Corn', 'Soybean', 'Rapeseed', 'Sunflowerseed'],
    description='Select the crop and check its risk index in {} days:'.format(N)
)

button = widgets.Button(description="Confirm Selection")
button.on_click(confirm_selection)

display(dropdown, button)

print("Risk in index for", selected_crop, "in {} days".format(N))

geolocator = Nominatim(user_agent="geoapi", timeout=10)

cache = {}

def clean_location(address):
    address = address.replace("b.", "bei")
    if "(FL)" in address:
        address = address.replace("(FL)", "").strip()
        country = "Liechtenstein"
    else:
        country = "Switzerland"
    return address, country

def get_lat_lon_cached(address):
    if address in cache:
        return cache[address]

    try:
        clean_addr, country = clean_location(address)
        location = geolocator.geocode(f"{clean_addr}, {country}")
        if location:
            result = (location.latitude, location.longitude)
        else:
            result = (None, None)
    except:
        result = (None, None)

    manual_fix = {
        "3292 Busswil bei Büren": (47.133, 7.246),
        "9494 Schaan": (47.166, 9.516)
    }
    if result == (None, None) and clean_addr in manual_fix:
        result = manual_fix[clean_addr]

    cache[address] = result
    return result

print(f"Processing {selected_crop}...")

coords_list = []
for loc in tqdm(locations_dict[selected_crop]):
    lat, lon = get_lat_lon_cached(loc)
    coords_list.append([loc, lat, lon])

selected_crop_loc = pd.DataFrame(coords_list, columns=["Standort", "Latitude", "Longitude"])

print(" ")
print("The locations of farms in Switzerland for {}".format(selected_crop))
print(" ")
print(selected_crop_loc)

"""***Input 2: Climate Data***

* Historical climate data
"""

weather_rows = []
columns_keep = ["tavg", "tmin", "tmax", "prcp", "snow", "wspd", "wpgt"]

for idx, row in selected_crop_loc.iterrows():
    lat, lon = row["Latitude"], row["Longitude"]
    start = datetime(year, 1, 1)
    end = datetime(year, 12, 31)
    stations = Stations().nearby(lat, lon).fetch(10)
    if stations.empty:
        print(f"No stations found for {lat},{lon}")
        continue
    station_means = []
    for sid in stations.index:
        try:
            data = Daily(sid, start, end).fetch()
            if data.empty:
                continue
            yearly_mean = data[columns_keep].mean()
            station_means.append(yearly_mean)
        except Exception as e:
            print(f"Failed for station {sid}: {e}")
    if not station_means:
        continue
    combined_mean = pd.concat(station_means, axis=1).mean(axis=1, skipna=True)
    weather_data = {
        "Standort": row["Standort"],
        "Latitude": lat,
        "Longitude": lon
    }
    weather_data.update(combined_mean.to_dict())
    weather_rows.append(weather_data)
    if len(weather_rows) >= loc_num:
        break

weather_df = pd.DataFrame(weather_rows)
if "snow" in weather_df.columns:
    weather_df["snow"].fillna(0.025, inplace=True)

print(weather_df)

"""***Collection of extrem weather days***

OPEN_METEO_ARCHIVE = "https://archive-api.open-meteo.com/v1/archive"

THRESH = {
    "heavy_rain_mm": 50.0,
    "heavy_snow_cm": 20.0,
    "heatwave_temp_c": 35.0,
    "strong_wind_ms": 15.0,
    "extreme_uv_index": 8.0,
    "dry_day_precip_mm": 1.0,
    "dry_fraction_for_drought": 0.8,
}

def get_yearly_weather_and_extremes(lat, lon, year, thresholds=None, timezone="Europe/Zurich"):
    if thresholds is None:
        thresholds = THRESH
    else:
        tmp = THRESH.copy()
        tmp.update(thresholds)
        thresholds = tmp

    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_date,
        "end_date": end_date,
        "daily": ",".join([
            "temperature_2m_max","temperature_2m_min",
            "precipitation_sum","snowfall_sum",
            "wind_speed_10m_max","uv_index_max"
        ]),
        "timezone": timezone
    }

    r = requests.get(OPEN_METEO_ARCHIVE, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    daily = data.get("daily", {})
    if not daily:
        raise ValueError("Open-Meteo returned no daily data for the given coordinates/year.")

    df = pd.DataFrame(daily)

    mean_data = {
        "mean_temperature_2m_max": df["temperature_2m_max"].mean(),
        "mean_temperature_2m_min": df["temperature_2m_min"].mean(),
        "mean_precipitation_sum": df["precipitation_sum"].mean(),
        "mean_snowfall_sum": df["snowfall_sum"].mean(),
        "mean_wind_speed_10m_max": df["wind_speed_10m_max"].mean(),
        "mean_uv_index_max": df["uv_index_max"].mean()
    }

    df["extreme_uv"] = df["uv_index_max"] >= thresholds["extreme_uv_index"]
    df["heavy_rain"] = df["precipitation_sum"] >= thresholds["heavy_rain_mm"]
    df["heavy_snow"] = df["snowfall_sum"]/10 >= thresholds["heavy_snow_cm"]  # 转为cm
    df["extreme_heat"] = df["temperature_2m_max"] >= thresholds["heatwave_temp_c"]
    df["strong_wind"] = df["wind_speed_10m_max"] >= thresholds["strong_wind_ms"]
    df["dry_day"] = df["precipitation_sum"] < thresholds["dry_day_precip_mm"]

    total_days = len(df)
    dry_days = int(df["dry_day"].sum())
    dry_fraction = dry_days / total_days if total_days > 0 else None
    simple_drought_flag = dry_fraction is not None and dry_fraction >= thresholds["dry_fraction_for_drought"]

    extreme_summary = {
        "period_days": total_days,
        "heavy_rain_days": int(df["heavy_rain"].sum()),
        "heavy_snow_days": int(df["heavy_snow"].sum()),
        "extreme_uv_days": int(df["extreme_uv"].sum()),
        "extreme_heat_days": int(df["extreme_heat"].sum()),
        "strong_wind_days": int(df["strong_wind"].sum()),
        "dry_days": dry_days,
        "dry_fraction": dry_fraction,
        "simple_drought_detected": bool(simple_drought_flag)
    }

    result = {**mean_data, **extreme_summary}
    return result

summary_rows = []
for idx, row in selected_crop_loc.iterrows():
    try:
        weather_summary = get_yearly_weather_and_extremes(
            lat=row["Latitude"], lon=row["Longitude"], year=year
        )
        row_data = {
            "Standort": row["Standort"],
            "Latitude": row["Latitude"],
            "Longitude": row["Longitude"]
        }
        row_data.update(weather_summary)
        summary_rows.append(row_data)
        time.sleep(1)
    except requests.exceptions.HTTPError as e:
        print(f"Request failed for {row['Standort']}: {e}")
        time.sleep(1)

extreme_weather_df = pd.DataFrame(summary_rows)
print(extreme_weather_df)

def get_forecast_weather(n_days):

    Fetch forecast daily weather for the next n_days.
    Converts official weathercode to simplified types like historical data.

    FORECAST_URL = "https://api.open-meteo.com/v1/forecast"
    start_date = date.today()
    end_date = start_date + timedelta(days=n_days-1)

    daily_vars = [
        "temperature_2m_max","temperature_2m_min",
        "precipitation_sum","windspeed_10m_max",
        "weathercode","snowfall_sum"
    ]

    params = {
        "latitude": latitude,
        "longitude": longitude,
        "daily": ",".join(daily_vars),
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),
        "timezone": "Europe/Zurich"
    }
    resp = requests.get(FORECAST_URL, params=params)
    if resp.status_code == 200:
        df = pd.DataFrame(resp.json()["daily"])

        # map official weathercode to simplified types
        def code_to_simple(wc, precip, wind, snow):
            if snow > 0:
                return "snow"
            elif precip > 5:
                return "rain"
            elif precip > 0 and wind > 10:
                return "showers/windy"
            elif wc in [0,1,2]:  # clear/sunny codes
                return "sunny"
            else:
                return "cloudy"

        df["weathercode"] = df.apply(lambda row: code_to_simple(
            row["weathercode"], row["precipitation_sum"], row["windspeed_10m_max"], row.get("snowfall_sum",0)
        ), axis=1)
        return df
    else:
        print("Forecast request failed:", resp.status_code)
        return None

print("=== Forecast Weather ===")
try:
    fc_df = get_forecast_weather(14)  # next 5 days
    if fc_df is not None:
        print(fc_df)
except Exception as e:
    print("Forecast weather fetch error:", e)

***Input 3: Yield Data (Optional but Recommended)***
"""

df = pd.read_csv("FAOSTAT_data.csv")

keywords = ["Wheat", "Barley", "Corn", "Soya", "Rapeseed", "Sunflower"]

mask = df["Item"].str.contains('|'.join(keywords), case=False, na=False)
df_filtered = df[mask]
columns_to_keep = ["Area", "Item", "Year", "Unit", "Value"]
df_filtered = df_filtered[columns_to_keep]
print(df_filtered)

"""***Input 4: Administrative or grid boundaries (crop location)***

"""

import ee
import geemap

# =========================
# 0️⃣ 初始化 Earth Engine
# =========================
ee.Authenticate()  # 只需运行一次，会弹出授权
ee.Initialize(project='trusty-sentinel-473919-a6')  # 填入你的项目ID

# =========================
# 1️⃣ 设置作物产地
# =========================
locations = [
    {"name": "Ersigen", "lat": 47.093969, "lon": 7.600372, "crop": "Barley"},
    {"name": "Düdingen", "lat": 46.849259, "lon": 7.187919, "crop": "Barley"},
    {"name": "Bercher", "lat": 46.691383, "lon": 6.708446, "crop": "Barley"}
    # 可继续补充其他地点
]

# =========================
# 2️⃣ 获取 Sentinel-2 遥感影像并计算 NDVI
# =========================
# 这里 filterBounds 先取全部地点的范围
all_points = ee.Geometry.MultiPoint([[loc["lon"], loc["lat"]] for loc in locations])

collection = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
              .filterDate("2024-04-01", "2024-09-30")  # 生长季
              .filterBounds(all_points)
              .map(lambda img: img.normalizedDifference(['B8', 'B4']).rename('NDVI')))

# =========================
# 3️⃣ 提取每个地点 NDVI
# =========================
def get_ndvi(loc):
    point = ee.Geometry.Point([loc["lon"], loc["lat"]])
    ndvi_ts = collection.mean().reduceRegion(
        reducer=ee.Reducer.mean(),
        geometry=point.buffer(500),  # 500m 缓冲区
        scale=10
    )
    ndvi_value = ndvi_ts.get("NDVI").getInfo()  # 可能是 None
    return {
        "name": loc["name"],
        "crop": loc["crop"],
        "ndvi": ndvi_value
    }

results = [get_ndvi(loc) for loc in locations]

# =========================
# 4️⃣ 用 NDVI 分配产量
# =========================
total_barley = 100000  # 瑞士大麦总产量
barley_points = [r for r in results if r["crop"] == "Barley"]

# 只保留 NDVI 有效的点
valid_points = [r for r in barley_points if r["ndvi"] is not None]

# 如果没有有效 NDVI，避免除以 0
if valid_points:
    ndvi_sum = sum(r["ndvi"] for r in valid_points)
    for r in valid_points:
        r["estimated_yield"] = (r["ndvi"] / ndvi_sum) * total_barley
else:
    print("⚠️ 所有地点 NDVI 都为 None！无法分配产量。")

# 打印结果
for r in valid_points:
    print(f"{r['name']}: NDVI={r['ndvi']:.3f}, Estimated Yield={r['estimated_yield']:.1f} t")